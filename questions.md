#### Open Questions
1. Why the aversion to negative weights in activation functions ?
2. How are we ensured to get different feature maps ?
3. What are various activation functions ? What are major similarities and differences ?
4. Why does overlapping of pooling layer improve generalizations ? [Alexnet]
5. Why does weight decay reduce model's training error ? [Alexnet]
